Model,Base Model,Description,Latest Update,Downloads last month,Language,Fine-tuned Dataset,Model Size,Parameters,Labels
cardiffnlp/twitter-roberta-base-sentiment-latest,RoBERTa-base,RoBERTa-base model trained on Twitter 2021 ~124M (RoBERTa-base) fine-tuned on TweetEval benchmark (~124M tweets),2022,"1,618,710",English,"tweet_eval ~66k tweets (train=45.6k,test=12.3k,val=2k)",500MB,125M,"Negative, Neutral, Positive"
cardiffnlp/twitter-xlm-roberta-base-sentiment,XLM-RoBERTa-base,Multilingual XLM-RoBERTa-base model trained on ~198M tweets and fine-tuned for sentiment analysis,2021,"1,179,935",Multilingual,"8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) tweets",1.3GB,270M,"Negative, Neutral, Positive"
cardiffnlp/twitter-roberta-base-sentiment,RoBERTa-base,RoBERTa-base model trained on Twitter ~58M (RoBERTa-base) fine-tuned on TweetEval benchmark (~58M tweets),2021,"771,567",English,tweet_eval ~66k tweets,500MB,125M,"LABEL_0, LABEL_1, LABEL_2"
finiteautomata/bertweet-base-sentiment-analysis,BERTweet-base,VinAIResearch/BERTweet model trained on Twitter 2012-2019 845M English Tweets and 5M COVID-19 Tweets,2023,"230,948",English,SemEval 2017 corpus (around ~40k tweets),530MB,135M,"NEG, NEU, POS"
